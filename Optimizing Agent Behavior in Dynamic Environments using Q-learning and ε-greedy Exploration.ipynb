{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6141cfa",
   "metadata": {},
   "source": [
    "# Optimizing Agent Behavior in Dynamic Environments using Q-learning and ε-greedy Exploration\n",
    "\n",
    "## Machine Learning Course - Reinforcement Learning Project \n",
    "\n",
    "## NCSR Demokritos 2022-2023\n",
    "\n",
    "### Author: Alexandros Filios - mtn2219"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835d000",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "In this report, we will present an implementation of a Q-learning, ε-greedy agent system for a repeated coordination game. The game is defined as a matrix with row column indexes called 1 and 2, where the payoff for each action combination is given. The agents belong to one of two types, X and Y, with X preferring action 1 over action 2 and Y preferring action 2 over action 1. The agents communicate with each other sparsely in an adjacency graph of our choice, and have no knowledge of the game or the repertoire of actions of the other agents.\n",
    "\n",
    "The Q-learning, ε-greedy algorithm is a popular method for solving multi-agent reinforcement learning problems, where the agents learn to make optimal decisions based on their own actions and the rewards received from the environment. The ε-greedy exploration strategy allows the agents to balance exploitation of their current knowledge with exploration of new actions.\n",
    "\n",
    "Our implementation consists of running a specified number of episodes, with the ε value starting at 1 and decreasing by 0.01 every X episodes. At the beginning of each episode, the time is set to 0, and each agent chooses an action using the ε-greedy strategy. After all agents have performed their actions, the environment announces the rewards to each agent, and the Q-values are updated using the Q-learning update rule. We track the energy value and the average total reward for each agent at each point in time, and create charts to visualize this data.\n",
    "\n",
    "The main goal of this report is to explore whether the agents converge to an equilibrium and how the \"equilibrium\" between X and Y agents affects their convergence. We will also include the code used, charts showing the energy value per episode, and charts for each agent type showing their average total reward at each point in time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2870bb8",
   "metadata": {},
   "source": [
    "## 2. Theoretical background & Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd5340",
   "metadata": {},
   "source": [
    "### 2.1 Q-Learning\n",
    "\n",
    "Q-learning is a type of reinforcement learning algorithm that is used to find the optimal action-selection policy for an agent in a specific environment. The basic idea behind Q-learning is to use a function, called the Q-function, to estimate the expected long-term reward for an agent when it takes a certain action in a given state. The Q-function is typically represented by a table or a neural network, and it is updated during the learning process to more accurately reflect the expected rewards for different actions in different states.\n",
    "\n",
    "The Q-function is defined as Q(s,a), where s is the current state, and a is the action taken in that state. The Q-function estimates the expected cumulative future reward for an agent starting from state s and taking action a. The agent's goal is to find the action-value function Q*(s,a) that maximizes the expected cumulative reward.\n",
    "\n",
    "In Q-learning, the agent interacts with the environment by taking actions and receiving rewards. At each time step, the agent selects an action based on its current state and the current estimates of the Q-function. The agent then receives a reward for the action and updates the Q-function to more accurately reflect the new information about the expected rewards for that action in that state. This process is repeated until the agent reaches a stopping criterion, such as a maximum number of iterations or a satisfactory level of performance.\n",
    "\n",
    "The Q-function is updated using the Q-learning update rule, which is a variation of the Bellman equation, a fundamental equation in dynamic programming. The Q-learning update rule states that the Q-value of a state-action pair is updated by taking the current Q-value and adjusting it by a small value proportional to the difference between the expected reward and the current estimate. The update rule is as follows:\n",
    "\n",
    "Q(s,a) = Q(s,a) + α(r + γ maxQ(s',a') - Q(s,a))\n",
    "\n",
    "Where:\n",
    "s: current state\n",
    "a: current action\n",
    "α: learning rate\n",
    "r: reward for taking action a in state s\n",
    "γ: discount factor\n",
    "s': next state\n",
    "a': next action\n",
    "maxQ(s',a'): maximum Q-value over all actions in next state s'\n",
    "\n",
    "The Q-function is used to select actions by choosing the action that has the highest Q-value for the current state. This is known as the greedy policy.\n",
    "\n",
    "### 2.2 ε-Greedy algorithm\n",
    "\n",
    "The ε-greedy algorithm is a method used to balance exploration and exploitation during the learning process. In the ε-greedy algorithm, the agent selects actions according to the following rule: with probability ε, a random action is chosen, and with probability 1-ε, the action that has the highest estimated Q-value is chosen. The value of ε is called the exploration rate and it's used to control the trade-off between exploration and exploitation. At the beginning of the learning process, ε is set to a high value to allow the agent to explore the environment and gather information about the different states and actions. As the learning process progresses, ε is gradually decreased, allowing the agent to become more exploitation-focused and make use of the information it has gathered.\n",
    "\n",
    "In summary, Q-learning is a model-free, off-policy algorithm for learning control policies, where the Q-function is used to approximate the optimal action-value function. It uses the Q-learning update rule which uses the Bellman equation and the agent interacts with the environment and learns by trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c860699",
   "metadata": {},
   "source": [
    "## 3. Implementation & Experinments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3f990b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m     episode_rewards[agent_type] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards[agent_type]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Update the energy value\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m energy_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Update the episode counter\u001b[39;00m\n\u001b[1;32m     82\u001b[0m episode_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2295\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2299\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the game matrix\n",
    "game_matrix = np.array([[1.1, 0.0], [0.0, 1.1]])\n",
    "\n",
    "# Define the number of agents and the types of agents\n",
    "num_agents = 7\n",
    "agent_types = [\"X\", \"Y\"]\n",
    "\n",
    "# Initialize the Q-values for each agent and each state-action pair to a small random value\n",
    "q_values = {agent_type: {i: np.random.rand(2) * 0.1 for i in range(num_agents)} for agent_type in agent_types}\n",
    "\n",
    "# Define the adjacency graph\n",
    "# For example, adjacency_graph[0][1] = True means agent 0 is connected to agent 1\n",
    "adjacency_graph = np.random.randint(0, 2, size=(num_agents, num_agents))\n",
    "\n",
    "# Define the number of episodes and the episode length\n",
    "num_episodes = 2500\n",
    "episode_length = 50\n",
    "\n",
    "# Define the discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Define the initial exploration probability and the number of episodes over which it decreases\n",
    "epsilon = 1\n",
    "epsilon_decay_episodes = 100\n",
    "\n",
    "# Initialize the rewards for each agent\n",
    "rewards = {agent_type: np.zeros(num_agents) for agent_type in agent_types}\n",
    "\n",
    "# Initialize the energy value\n",
    "energy_value = 0\n",
    "\n",
    "# Initialize the episode counter\n",
    "episode_counter = 0\n",
    "\n",
    "# Run the episodes\n",
    "for episode in range(num_episodes):\n",
    "    # Update the exploration probability\n",
    "    if episode_counter % epsilon_decay_episodes == 0 and episode_counter != 0:\n",
    "        epsilon -= 0.01\n",
    "    \n",
    "    # Initialize the episode rewards\n",
    "    episode_rewards = {agent_type: np.zeros(num_agents) for agent_type in agent_types}\n",
    "    \n",
    "    # Run the episode steps\n",
    "    for step in range(episode_length):\n",
    "        # Initialize the actions for each agent\n",
    "        actions = {agent_type: np.zeros(num_agents) for agent_type in agent_types}\n",
    "        \n",
    "        # Get the actions for each agent\n",
    "        for i in range(num_agents):\n",
    "            agent_type = agent_types[i % 2]\n",
    "            if np.random.rand() < epsilon:\n",
    "                # Choose a random action with probability epsilon\n",
    "                actions[agent_type][i] = np.random.randint(2)\n",
    "            else:\n",
    "                # Choose the action with the highest Q-value with probability 1-epsilon\n",
    "                actions[agent_type][i] = np.argmax(q_values[agent_type][i])\n",
    "        \n",
    "        # Compute the rewards for each agent\n",
    "        for i in range(num_agents):\n",
    "            agent_type = agent_types[i % 2]\n",
    "            agent_rewards = np.zeros(num_agents)\n",
    "            for j in range(num_agents):\n",
    "                if adjacency_graph[i][j]:\n",
    "                    agent_rewards[j] = game_matrix[int(actions[agent_type][i]), int(actions[agent_types[j % 2]][j])]\n",
    "            rewards[agent_type][i] = np.mean(agent_rewards)\n",
    "    # Update the Q-values for each agent\n",
    "    for i in range(num_agents):\n",
    "        agent_type = agent_types[i % 2]\n",
    "        q_values[agent_type][i][int(actions[agent_type][i])] = rewards[agent_type][i] + gamma * np.max(q_values[agent_type][i])\n",
    "\n",
    "    # Update the episode rewards for each agent\n",
    "    for agent_type in agent_types:\n",
    "        episode_rewards[agent_type] += rewards[agent_type]\n",
    "\n",
    "    # Update the energy value\n",
    "    energy_value += np.mean(np.sum(episode_rewards, axis=1))\n",
    "\n",
    "    # Update the episode counter\n",
    "    episode_counter += 1\n",
    "\n",
    "    # Plot the energy value and the average total rewards for each agent type\n",
    "    # Code for plotting goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b92026",
   "metadata": {},
   "source": [
    "## 4. Epilogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fdfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
